{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed798388-c907-4905-9e5d-76891490b222",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6248e53-5259-4bea-bc6b-0ac9e66fd64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch --quiet --quiet\n",
    "%pip install coba --quiet --quiet\n",
    "%pip install scipy --quiet --quiet\n",
    "%pip install numpy --quiet --quiet\n",
    "%pip install matplotlib --quiet --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc665e0-f5ee-4e4f-a478-72b9b0ebe4f0",
   "metadata": {},
   "source": [
    "### TODO\n",
    "+ Change all 'reward' names to 'loss'\n",
    "+ Move MyRewardPredictor back into this notebook\n",
    "+ Have the readme suggest how to make a new conda environment and install jupyter lab\n",
    "+ Make sure to remove test.py from the repo\n",
    "+ Rip out all coba dependencies from examples such as cb.Batch\n",
    "+ Ensure numpy rather than ensure tensor for vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23509513-8f3c-4383-aa54-f4326d182fc5",
   "metadata": {},
   "source": [
    "### Run Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958455b-65bb-4dfb-b0f6-ef8a4fa681e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 13:15:52 -- Experiment Started\n",
      "2023-07-19 13:15:52 -- Recording Learner 0 parameters... (0.0 seconds) (completed)\n",
      "2023-07-19 13:15:52 -- Recording Evaluator 0 parameters... (0.0 seconds) (completed)\n",
      "2023-07-19 13:15:52 -- Recording Environment 0 parameters... (0.0 seconds) (completed)\n",
      "2023-07-19 13:15:57 -- Peeking at Environment 0... (5.21 seconds) (completed)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import coba as cb\n",
    "import numpy as np\n",
    "from CappedIGW import CappedIGW\n",
    "from ScaledL1Loss import MakeLosses\n",
    "from PowerScheduler import PowerScheduler\n",
    "from MyRewardPredictor import MyRewardPredictor\n",
    "from UniformReferencePolicy import UniformReferencePolicy\n",
    "\n",
    "#these values are specific to openml dataset 41540\n",
    "n_context_dim, n_action_dim = 22, 1\n",
    "tzero = 100\n",
    "gamma_tzero = 1e-4\n",
    "lr = 1e-2\n",
    "batch_size = 8\n",
    "n_batches = 2_500\n",
    "\n",
    "fhat = MyRewardPredictor(\n",
    "    numrff=1024,\n",
    "    sigma=2e-1,\n",
    "    in_features=n_context_dim+n_action_dim,\n",
    "    opt_factory=lambda params: torch.optim.Adam(params,lr=lr),\n",
    "    sched_factory=lambda opt: torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=PowerScheduler(tzero,-.5))\n",
    ")\n",
    "\n",
    "cb.Environments.cache_dir('.coba_cache')\n",
    "\n",
    "env = cb.Environments.from_openml(data_id=41540,take=batch_size*n_batches).scale().filter(MakeLosses()).batch(batch_size)\n",
    "lrn = CappedIGW(mu=UniformReferencePolicy(), \n",
    "                fhat=fhat, \n",
    "                gamma_scheduler = PowerScheduler(gamma_tzero,.5))\n",
    "\n",
    "result = cb.Experiment(env,lrn).run()\n",
    "result.plot_learners(y='reward'                  , span=2_000, xlim=(100,None))\n",
    "result.plot_learners(y='reward_prediction_regret', span=2_000, xlim=(100,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21773fcf-e24f-4ac6-9ee1-c772c99fb181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
