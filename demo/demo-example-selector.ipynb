{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78c777a-cc5d-4d16-b3c1-38d0cee317a7",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "In this notebook you can play with a combinatorial optimization problem we solve with CappedIGW. This example is not described in the original paper, but shows the potential for CappedIGW for in-context learning with LLM models.\n",
    "\n",
    "In the notebook below we define a reference policy and a loss predictor for this dataset. To use CappedIGW this is all you have to define. The rest of the algorithm can be treated as a black box.\n",
    "\n",
    "For the reference policy below we use embeddings from a transformer to calculate similarity scores. We then sample from a softmax distribution of the similarity ranking using a Gumbel distribution.\n",
    "\n",
    "A quick note, throughout the notebook we use Coba to orchestrate our experiments and record their results. It is not necessary to learn about or use coba to understand and utilize CappedIGW beyond this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed798388-c907-4905-9e5d-76891490b222",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6248e53-5259-4bea-bc6b-0ac9e66fd64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch --quiet --quiet\n",
    "%pip install coba --quiet --quiet\n",
    "%pip install scipy --quiet --quiet\n",
    "%pip install numpy --quiet --quiet\n",
    "%pip install matplotlib --quiet --quiet\n",
    "%pip install cloudpickle --quiet --quiet\n",
    "%pip install sentence-transformers --quiet --quiet\n",
    "%pip install ipywidgets --quiet --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23509513-8f3c-4383-aa54-f4326d182fc5",
   "metadata": {},
   "source": [
    "### Define Embeddings, Reference Policy, Loss Predictor, and Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a4f1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from typing import Sequence, Callable\n",
    "\n",
    "import torch\n",
    "import coba as cb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from AbstractClasses import ReferencePolicy, LossPredictor\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embedder(items,device=None):\n",
    "    if not isinstance(items,(str,list)):\n",
    "        raise AssertionError()\n",
    "    if isinstance(items,str):\n",
    "        items = [items]\n",
    "    if isinstance(items,list):\n",
    "        items = [ i['instruction'] for i in items]\n",
    "\n",
    "    embeddings = model.encode(items,convert_to_tensor=True).to(device=device)\n",
    "    normalized = torch.nn.functional.normalize(embeddings)\n",
    "\n",
    "    return normalized\n",
    "\n",
    "def stratum(item):\n",
    "    text = item if isinstance(item,str) else item['instruction']\n",
    "    return 'first' if 'first' in text else 'second' if 'second' in text else 'last'\n",
    "\n",
    "class RandomizedSimilarity(ReferencePolicy):\n",
    "    def __init__(self, \n",
    "        embedder: Callable, \n",
    "        examples: Sequence, \n",
    "        ex_embeddings: Sequence, \n",
    "        batch_size: int, \n",
    "        temperature:float,\n",
    "        set_size:int,\n",
    "        stratum: Callable = lambda item: 1,\n",
    "        preselect:int = 500) -> None:\n",
    "        self._embedder = embedder\n",
    "        self._batch_size = batch_size\n",
    "        self._temperature = temperature\n",
    "        self._set_size = set_size\n",
    "        self._stratum = stratum\n",
    "        self._preselect = preselect\n",
    "\n",
    "        self._strata_examples = defaultdict(list)\n",
    "        self._strata_embeddings = defaultdict(list)\n",
    "        for example,embedding in zip(examples,ex_embeddings):\n",
    "            self._strata_examples[stratum(example)].append(example)\n",
    "            self._strata_embeddings[stratum(example)].append(embedding)\n",
    "        self._strata_examples.default_factory = None\n",
    "        self._strata_embeddings.default_factory = None\n",
    "\n",
    "        for stratum,embeddings in self._strata_embeddings.items():\n",
    "            self._strata_embeddings[stratum] = torch.stack(embeddings)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return {'temp':self._temperature, 'sampler':'RandomizedSimilarity', 'n_strata':len(self._strata_examples)}\n",
    "    \n",
    "    def sample(self, context):\n",
    "        with torch.no_grad():\n",
    "            context_stratum = self._stratum(context)\n",
    "    \n",
    "            embeddings = self._strata_embeddings[context_stratum]\n",
    "            examples   = self._strata_examples[context_stratum]\n",
    "            \n",
    "            embedded_context = self._embedder(context)\n",
    "            all_similarities = embedded_context @ embeddings.T\n",
    "            top_similarities = torch.topk(all_similarities,k=self._preselect)\n",
    "            similarities     = top_similarities.values\n",
    "            original_indices = top_similarities.indices\n",
    "            \n",
    "            gumbel = torch.distributions.gumbel.Gumbel(0,1)\n",
    "            gumbel_shape = torch.Size([self._batch_size, similarities.shape[1]])\n",
    "\n",
    "            while True:\n",
    "                gumbels = gumbel.sample(gumbel_shape)*self._temperature\n",
    "                topks   = torch.topk(similarities+gumbels,self._set_size,dim=1).indices\n",
    "                \n",
    "                yield [ [ (examples[original_indices[0,i]],similarities[0,i].item()) for i in row] for row in topks ]\n",
    "\n",
    "class MyLossPredictor(LossPredictor):\n",
    "    class LogisticRegressor(torch.nn.Module):\n",
    "        def __init__(self, in_features:int):\n",
    "            super().__init__()\n",
    "            self.linear  = torch.nn.Linear(in_features=in_features, out_features=1)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        @property\n",
    "        def params(self):\n",
    "            return {'type':'logistic'}\n",
    "    \n",
    "        def pre_logits(self, X):\n",
    "            return self.linear(X)\n",
    "\n",
    "        def loss(self, X):\n",
    "            return self.sigmoid(self.pre_logits(X))\n",
    "\n",
    "    def __init__(self, *, set_size:int, opt_factory, sched_factory, params={}) -> None:\n",
    "        self._regressor = MyLossPredictor.LogisticRegressor(4*set_size)\n",
    "        self.loss       = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "        self.opt        = opt_factory(self._regressor.parameters())\n",
    "        self.scheduler  = sched_factory(self.opt)\n",
    "        self.y_sum      = 0\n",
    "        self.t          = 0\n",
    "        self._params    = params\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return {**self._regressor.params, **self._params}\n",
    "\n",
    "    def _features(self,x,a):\n",
    "        features = []\n",
    "        for e in a:\n",
    "            i = e[0]['instruction']\n",
    "\n",
    "            x_task = 'first' if 'first' in x else 'second' if 'second' in x else 'last'\n",
    "            e_task = 'first' if 'first' in i else 'second' if 'second' in i else 'last'\n",
    "\n",
    "            x_name = x[x.find('\"')+1:x.rfind('\"')]\n",
    "            e_name = i[i.find('\"')+1:i.rfind('\"')]\n",
    "\n",
    "            x_first,x_last = x_name.split(' ')\n",
    "            e_first,e_last = e_name.split(' ')\n",
    "\n",
    "            same_task  = int(x_task==e_task)\n",
    "            same_first = int(x_first==e_first)\n",
    "            same_last  = int(x_last==e_last)\n",
    "            similarity = e[1]\n",
    "\n",
    "            features.extend([same_task,same_first,same_last,similarity])\n",
    "        return features\n",
    "\n",
    "    def predict(self, context: str, actions) -> Sequence[float]:\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor([ self._features(context,action) for action in actions])\n",
    "            return self._regressor.loss(X)\n",
    "\n",
    "    def learn(self, contexts: torch.Tensor, actions: Sequence[float], losses: Sequence[float]) -> None:\n",
    "        self.t += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor([self._features(context,action) for context,action in zip(contexts,actions)])            \n",
    "            y = torch.tensor(losses).float()\n",
    "            self.y_sum += torch.mean(y).item()\n",
    "        \n",
    "        self.opt.zero_grad()\n",
    "        yhat = self._regressor.pre_logits(X).squeeze(1)\n",
    "        loss = self.loss(yhat,y)\n",
    "        loss.mean().backward()\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('ignore')\n",
    "            self.opt.step()\n",
    "            self.scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_av = self.y_sum/self.t\n",
    "            best_const_loss = 0 if y_av <= 0 or y_av >= 1 else self.loss(torch.logit(y_av*torch.ones_like(y)),y)\n",
    "            cb.CobaContext.learning_info['loss_prediction_loss'] = loss.tolist()\n",
    "            cb.CobaContext.learning_info['loss_prediction_regret'] = (loss-best_const_loss).tolist()\n",
    "\n",
    "class FewShotFixedStrategy:\n",
    "    def __init__(self, sampler:ReferencePolicy) -> None:\n",
    "        self._sampler = sampler\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self._sampler.params\n",
    "    def predict(self, context, actions):\n",
    "        if isinstance(context,cb.Batch): raise Exception()\n",
    "        action, prob = next(self._sampler.sample(context))[0],None\n",
    "        return action, prob\n",
    "    def learn(self, context, actions, action, reward, probs, **kwargs):\n",
    "        pass\n",
    "\n",
    "class ZeroShotStrategy:\n",
    "    def predict(self, context, actions):\n",
    "        if isinstance(context,cb.Batch): raise Exception()\n",
    "        return [],None\n",
    "    def learn(self, context, actions, action, reward, probs, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14bb7e7-f5ad-4105-ab86-e09d293fc8eb",
   "metadata": {},
   "source": [
    "### Run Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958455b-65bb-4dfb-b0f6-ef8a4fa681e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import coba as cb\n",
    "from LetCatEnvironment import LetCatEnvironment\n",
    "from CappedIGW import CappedIGW\n",
    "\n",
    "tzero = 100\n",
    "gamma_tzero = 1e-1\n",
    "lr = 1e-2\n",
    "batch_size = 8\n",
    "n_batches = 2\n",
    "\n",
    "with open('LetCatTrain.jsonl',mode='rb') as f:\n",
    "    examples = [ json.loads(line) for line in f ][:5000]\n",
    "ex_embeddings = embedder(examples)\n",
    "\n",
    "rs_00_1_strat = RandomizedSimilarity(embedder, examples, ex_embeddings, batch_size=1 , temperature=.00, set_size=3, stratum=stratum)\n",
    "rs_00_1       = RandomizedSimilarity(embedder, examples, ex_embeddings, batch_size=1 , temperature=.00, set_size=3)\n",
    "rs_05_1       = RandomizedSimilarity(embedder, examples, ex_embeddings, batch_size=1 , temperature=.05, set_size=3)\n",
    "rs_05_30      = RandomizedSimilarity(embedder, examples, ex_embeddings, batch_size=30, temperature=.05, set_size=3)\n",
    "\n",
    "fhat = MyLossPredictor(\n",
    "    set_size=3,\n",
    "    opt_factory=lambda params: torch.optim.Adam(params,lr=lr),\n",
    "    sched_factory=lambda opt: torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda t:(1+t/tzero)**(-.5))\n",
    ")\n",
    "\n",
    "env = cb.Environments(LetCatEnvironment()).take(n_batches*batch_size).batch(batch_size)\n",
    "lrn = [\n",
    "    FewShotFixedStrategy(rs_00_1),\n",
    "    FewShotFixedStrategy(rs_05_1),\n",
    "    ZeroShotStrategy(),\n",
    "    CappedIGW(mu=rs_05_30, fhat=fhat, gamma_sched=lambda t: (1 + t/gamma_tzero)**(0.5))\n",
    "]\n",
    "\n",
    "result = cb.Experiment(env,lrn).run(processes=5,quiet=True)\n",
    "result.plot_learners(out=None)\n",
    "result.where(learner_id=0).plot_learners(y='best_const',colors=1+len(lrn),labels='Best Constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417e7a7-e60b-4d87-bf72-6d28a8ffedfc",
   "metadata": {},
   "source": [
    "### Run Hyperparameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86f0bc7-67b0-450f-8329-a121adb21333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import functools\n",
    "\n",
    "take        = 10_000\n",
    "n_processes = 8\n",
    "n_samples   = 60\n",
    "log         = 'sweep.log'\n",
    "device      = 'cpu'\n",
    "\n",
    "_embedder = lambda items, device=device: embedder(items,device) \n",
    "\n",
    "if n_processes > 1:\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "\n",
    "def uniform(low,high,n) -> torch.Tensor:\n",
    "    return torch.distributions.uniform.Uniform(low,high).sample([n])\n",
    "\n",
    "def loguniform(low, high, n) -> torch.Tensor:\n",
    "    return uniform(*torch.tensor([low,high]).log(),n).exp()\n",
    "\n",
    "def generate_random_hypers(n):\n",
    "    lrs  = loguniform(1e-3,1e1,n).tolist()\n",
    "    tzs  = loguniform(1e-1,1e4,n).tolist()\n",
    "    gtzs = loguniform(1e-5,1e2,n).tolist()\n",
    "\n",
    "    yield from zip(lrs,tzs,gtzs)\n",
    "\n",
    "with open('LetCatTrain.jsonl',mode='rb') as f:\n",
    "    examples = [ json.loads(line) for line in f ][:5000]\n",
    "ex_embeddings = _embedder(examples)\n",
    "\n",
    "rs_00_1  = RandomizedSimilarity(_embedder, examples, ex_embeddings, batch_size=1 , temperature=.00, set_size=3)\n",
    "rs_05_30 = RandomizedSimilarity(_embedder, examples, ex_embeddings, batch_size=30, temperature=.05, set_size=3)\n",
    "\n",
    "env = cb.Environments(LetCatEnvironment()).take(take).batch(1)\n",
    "val = cb.OnPolicyEvaluator()\n",
    "lrn = [ FewShotFixedStrategy(rs_00_1) ]\n",
    "\n",
    "for lr,tz,gtz in generate_random_hypers(n_samples):\n",
    "    fhat = MyLossPredictor(\n",
    "        set_size=3,\n",
    "        opt_factory=lambda params: torch.optim.Adam(params,lr=lr),\n",
    "        sched_factory=lambda opt: torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda t:(1+t/tz)**(-.5)),\n",
    "        params = {'lr':lr, 'tz':tz, 'gtz':gtz}\n",
    "    )\n",
    "    lrn.append(CappedIGW(mu=rs_05_30, fhat=fhat, gamma_sched=lambda t:(1+t/gtz)**(0.5)))\n",
    "\n",
    "result = cb.Experiment(env,lrn).run(log,processes=n_processes,quiet=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
